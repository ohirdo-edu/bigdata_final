 # Робота з великими даними
 
 - Обрати датасет (достатньо великий)
 - Обрати задачу, що потребує виконання декілької етапів MapReduce фаз (не менше 2 повних фаз) та демонструє ваше розуміння парадигми
 - Розписати алгоритм
 - Реалізувати з використанням MapReduce (класичний, Hive, Pig, Spark)

 # Виконання

 Обраний датасет -- дані [про поїздки на таксі](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).

 [Опис датасету](./data_dictionary_trip_records_hvfhs.pdf)

 Обрана задача -- визначити райони, де водіям дають найбільше чайових.

 Опис алгоритму та реалізація на PySpark -- в [ноутбуці](./shared/main.ipynb)
 